# 模型

【Word2Vec】
- [Word2Vec的参数解释](https://blog.csdn.net/laobai1015/article/details/86540813)
- [Word2Vec的簡易教學與參數調整指南](https://www.kaggle.com/jerrykuo7727/word2vec)

【Fasttext】
- [官方模型链接](https://github.com/facebookresearch/fastText/blob/master/docs/crawl-vectors.md#models)

【TextCNN】
- [TextCNN的优化经验](https://www.cnblogs.com/ModifyRong/p/11442661.html)

【RNN】
- [RNN GRU LSTM参数计算](https://blog.csdn.net/imhuay/article/details/80267269)

【Transformer】
- [Transformer论文翻译](https://so.csdn.net/so/search/s.do?q=Transformer%E8%AE%BA%E6%96%87%E8%AF%A6%E8%A7%A3%EF%BC%8C%E8%AE%BA%E6%96%87%E5%AE%8C%E6%95%B4%E7%BF%BB%E8%AF%91&t=&o=&s=&tm=&v=&l=&lv=&u=&ft=)
- [放弃幻想，全面拥抱Transformer](https://zhuanlan.zhihu.com/p/54743941)
- [Transformer和Bert相关知识解答](https://zhuanlan.zhihu.com/p/149634836)
- [NLP中 batch normalization与 layer normalization](https://zhuanlan.zhihu.com/p/74516930)
- [关于batch normalization和layer normalization的理解](https://blog.csdn.net/HUSTHY/article/details/106665809)
- [超细节的 BERT/Transformer 知识点](https://cloud.tencent.com/developer/article/1748590)
- Transformer使用Adam优化器

【Bert】
- [BERT模型原理详解系列](https://zhuanlan.zhihu.com/p/46652512)
- [超细节的BERT/Transformer知识点](https://zhuanlan.zhihu.com/p/132554155)
- [BERT是如何分词的](https://cloud.tencent.com/developer/article/1524436)
- [BERT encoder参数量计算](https://lsc417.com/2020/06/19/bert_parameter/)
- [CPU上的Bert性能优化](https://blog.roblox.com/2020/05/scaled-bert-serve-1-billion-daily-requests-cpus/)
- [跨语种语言模型](https://zhuanlan.zhihu.com/p/139630839)
- [后BERT时代：15个预训练模型对比分析与关键点探究](https://cloud.tencent.com/developer/article/1491568)

Bert的位置编码可以有PE，正弦余弦编码。还可以使用层次分解位置编码来处理超长文本
- [层次分解位置编码，让BERT可以处理超长文本](https://mp.weixin.qq.com/s/bSOHfuwKZRY7WkqfXVkAPA)

为什么不需要decoder：作为一个预训练的模型，我们只需要学习到句子对应的向量，然后给下游任务使用
- [详解谷歌最强NLP模型BERT](https://www.mdeditor.tw/pl/2ihW)

在具体的NLP任务上，BERT模型的输入输出会有细微的差别
- [图示详解BERT模型的输入与输出](https://www.cnblogs.com/gczr/p/11785930.html)

- Bert的前馈层使用Gelu激活函数

【Bert变种】
- [RoBERTa 和 ALBERT](https://www.jianshu.com/p/769e66e085fe)