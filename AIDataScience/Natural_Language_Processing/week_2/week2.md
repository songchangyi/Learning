# Language modeling and sequence tagging

## Language modeling: it's all about counting!
### Math
- Chain rule : P(w) = P(w1) * P(w2|w1)...P(Wk|w1...w(k-1))
- Markov assumption : P(wi|w1w2...w(i-1)) = P(wi|w(i-n+1)...w(i-1))

### Bigram LM
P(w) = P(w1)P(w2|w1)...P(w(k)|w(k-1))

Problem : normalized separately for each sequence length

P(w) = P(w1|start)P(w2|w1)...P(end|w(k-1))

P(w(i)|w(i-1)) = c(w(i-1)wi) / c(w(i-1))

## Perplexity: is our model surprised with a real text?
### N-gram language model
P(w) = (1:k+1) P(w(i)|w(i-n+1)...w(i-1))

### Log-likelihood maximization

### Extrinsic evaluation

### Intrinsic evaluation
- perplexity
  - the lower the better
  - Use UNK for OOV
  
## Smoothing: what if we see new n-grams?
### Laplacian smoothing
- add-one smoothing : c(w(i)(i-n+1))+1 / c(w(i-1)(i-n+1))+V
- add-k smoothing : c(w(i)(i-n+1))+k / c(w(i-1)(i-n+1))+Vk

### Katz backoff
Try a longer n-gram and back off to shorter if needed

### Interpolation smoothing
lambda1P1 + lambda2P2 + lambda3P3

### Absolute discounting
Compare the counts for bigrams in train and test sets.

### Kneser-Ney smoothing
We need to capture the diversity of contexts of the world.

# Sequence tagging with probabilistic models
## Hidden Markov Models
### Problem
Given a sequence of tokens, infer the most probable sequence of labels for these tokens.

Ex. 
- part-of-speech (POS) tagging
  - Universal Dependencies project
- named entity recognition
- semantic slot filling

### Types of named entities
- persons, organizations, locations...
- dates and times, units, amounts...

### Approaches to sequence labeling
1. Rule-based models (EngCG tagger)
2. Separate label classifiers for each token (NB, LR)
3. Sequence models (HMM, MEMM, CRF)
4. NN

### PoS tagging with HMMs
Notation :
- x = x1...xt is a sequence of words (input)
- y = y1...yt is a sequence of tags (labels)

y = argmax P(y|x) = argmax P(x, y) (x is independent to y)

### Hidden Markov Model
![p(x,y)=p(x|y)p(y)\approx  \prod_{t=1}^T p(x_t|y_t)p(y_t|y_{t-1})](https://render.githubusercontent.com/render/math?math=p(x%2Cy)%3Dp(x%7Cy)p(y)%5Capprox%20%20%5Cprod_%7Bt%3D1%7D%5ET%20p(x_t%7Cy_t)p(y_t%7Cy_%7Bt-1%7D))

2 assumptions:
1. Markov assumption : 
![p(y)\approx  \prod_{t=1}^T p(y_t|y_{t-1})](https://render.githubusercontent.com/render/math?math=p(y)%5Capprox%20%20%5Cprod_%7Bt%3D1%7D%5ET%20p(y_t%7Cy_%7Bt-1%7D))

2. Output independence : 
![p(x|y) \approx \prod_{t=1}^T p(x_t|y_t))](https://render.githubusercontent.com/render/math?math=p(x%7Cy)%20%5Capprox%20%5Cprod_%7Bt%3D1%7D%5ET%20p(x_t%7Cy_t)))

### Text generation in HMM
- choose the next PoS tag given the previous tag
- given the current tag, generates another word

### Formal definition of HMM
A hidden Markov Model is specified by
1. The set S = s1...sN of hidden states
2. The start state s0
3. The matrix A of transition probabilities : a_ij = p(s_j|s_i)
4. The set O of possible visible outcomes
5. The matrix B of output probabilities : b_ik = p(o_k|s_i)

a_ij = p(s_j|s_i) = c(s_i -> s_j)/c(s_i)

a_ik = p(o_k|s_i) = c(s_i -> o_k)/c(s_i)

### Baum-Welch algorithm (a sketch)
E-step : posterior probabilities for hidden variables : 
![p(y_{t-1}=s_i,y_t=s_j)](https://render.githubusercontent.com/render/math?math=p(y_%7Bt-1%7D%3Ds_i%2Cy_t%3Ds_j))

Can be done with dynamic programming (forward-backward algorithm)

M-step : maximum likelihood updates for the parameters
![a_ij=p(s_j|s_i)= \frac{\sum_{t=1}^T p(y_{t-1}=s_i,y_t=s_j)}{\sum_{t=1}^T p(y_t=s_i)} ](https://render.githubusercontent.com/render/math?math=a_ij%3Dp(s_j%7Cs_i)%3D%20%5Cfrac%7B%5Csum_%7Bt%3D1%7D%5ET%20p(y_%7Bt-1%7D%3Ds_i%2Cy_t%3Ds_j)%7D%7B%5Csum_%7Bt%3D1%7D%5ET%20p(y_t%3Ds_i)%7D%20)

## Viterbi algorithm: what are the most probable tags?
### Motivation
The same output sentence can be generated by different sequences of hidden states.
### Decoding in HMM
Decoding problem : what is the most probable sequence of hidden states => use dynamic programming
### Viterbi decoding
Find the most possible path. At each state, compare all possible ways and only keep the most possible one.

Complexity : change O(n^m) to O(m * n^2)

## MEMMs, CRFs and other sequential models for Named Entity Recognition
### HMM
![p(y,x)=\prod_{t=1}^T p(y_t|y_{t-1})p(x_t|y_t)](https://render.githubusercontent.com/render/math?math=p(y%2Cx)%3D%5Cprod_%7Bt%3D1%7D%5ET%20p(y_t%7Cy_%7Bt-1%7D)p(x_t%7Cy_t))

- Generative model
- Label_y => Output_y, Label_{y-1} => Label_y
### MEMM (Maximum Entropy Markov Model)
![p(y|x)=\prod_{t=1}^T p(y_t|y_{t-1},x_t)](https://render.githubusercontent.com/render/math?math=p(y%7Cx)%3D%5Cprod_%7Bt%3D1%7D%5ET%20p(y_t%7Cy_%7Bt-1%7D%2Cx_t))

- Discriminative model
- x_i + y_{i-1} => y_i
### CRF (Conditional Random Field)
![p(y|x)= \frac{1}{Z(x)} \prod_{t=1}^T exp(\sum_{k=1}^K  \theta_k f_k(y_t, y_{t-1}, x_t))](https://render.githubusercontent.com/render/math?math=p(y%7Cx)%3D%20%5Cfrac%7B1%7D%7BZ(x)%7D%20%5Cprod_%7Bt%3D1%7D%5ET%20exp(%5Csum_%7Bk%3D1%7D%5EK%20%20%5Ctheta_k%20f_k(y_t%2C%20y_%7Bt-1%7D%2C%20x_t)))

- Discriminative model
- Need more calculate to get Z(x)
- Undirected graph
- Black-box implementations : CRF++, MALLET, GRMM, CRFSuite, FACTORIE
### Features engineering
capital letters, predefined word list, etc.

# Deep Learning for the same tasks

## Neural Language Models
### Recap : Language modeling
### Curse of dimensionality
One hot encoding is not good
### How to generalize better
- Learn distributed representations for words : C^(|V| * m)
- Express probabilities of sequences and learn parameters
### Probabilistic Neural Language Model
- Softmax over components of y
- Feed-forward NN with tons of parameters : y = b + Wx + Utanh(d+Hx)
  - y_dim = V * 1
  - b_dim = V * 1
  - W_dim = V * (m * (n-1))
  - x_dim = (n-1) * 1
- Distributed representation of context words : x = [C(w_{i-n+1}),...C(w{i-1})]^T
### Log-Bilinear Language Model
- much less params and non-linear activations
- measures similarity between the word and the context
- Representation of word :
![r_{w_i}=C(w_i)^T](https://render.githubusercontent.com/render/math?math=r_%7Bw_i%7D%3DC(w_i)%5ET)
- Representation of context :
![\widehat{r}=\sum_{k=1}^{n-1} W_k C(w_{i-k})^T ](https://render.githubusercontent.com/render/math?math=%5Cwidehat%7Br%7D%3D%5Csum_%7Bk%3D1%7D%5E%7Bn-1%7D%20W_k%20C(w_%7Bi-k%7D)%5ET%20)

## Whether you need to predict a next word or a label - LSTM is here to help!
### Recap : RNN
![h_i=f(Wh_{i-1}+Vx_i+b)](https://render.githubusercontent.com/render/math?math=h_i%3Df(Wh_%7Bi-1%7D%2BVx_i%2Bb))

![y_i=Uh_i+ \widetilde{b} ](https://render.githubusercontent.com/render/math?math=y_i%3DUh_i%2B%20%5Cwidetilde%7Bb%7D%20)

### Train
Cross-entropy loss (for one position)
### Use
- Feed the previous output as the next input
- Take argmax at each step or use beam search (always 5 sequences)
### RNN Language Model
- Lower perplexity and word error rate than 5-gram model with Knesser-Ney smoothing
- Char level RNNs can be very effective
### Bi-directional LSTM
- Universal approach for sequence tagging
- stack layers + add layers on top
- trained by cross-entropy loss

Sequences tagging tasks : Bi-directional LSTM/CRF
