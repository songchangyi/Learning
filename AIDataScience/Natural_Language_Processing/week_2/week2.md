# Language modeling and sequence tagging

## Language modeling: it's all about counting!
### Math
- Chain rule : P(w) = P(w1) * P(w2|w1)...P(Wk|w1...w(k-1))
- Markov assumption : P(wi|w1w2...w(i-1)) = P(wi|w(i-n+1)...w(i-1))

### Bigram LM
P(w) = P(w1)P(w2|w1)...P(w(k)|w(k-1))

Problem : normalized separately for each sequence length

P(w) = P(w1|start)P(w2|w1)...P(end|w(k-1))

P(w(i)|w(i-1)) = c(w(i-1)wi) / c(w(i-1))

## Perplexity: is our model surprised with a real text?
### N-gram language model
P(w) = (1:k+1) P(w(i)|w(i-n+1)...w(i-1))

### Log-likelihood maximization

### Extrinsic evaluation

### Intrinsic evaluation
- perplexity
  - the lower the better
  - Use UNK for OOV
  
## Smoothing: what if we see new n-grams?
### Laplacian smoothing
- add-one smoothing : c(w(i)(i-n+1))+1 / c(w(i-1)(i-n+1))+V
- add-k smoothing : c(w(i)(i-n+1))+k / c(w(i-1)(i-n+1))+Vk

### Katz backoff
Try a longer n-gram and back off to shorter if needed

### Interpolation smoothing
lambda1P1 + lambda2P2 + lambda3P3

### Absolute discounting
Compare the counts for bigrams in train and test sets.

### Kneser-Ney smoothing
We need to capture the diversity of contexts of the world.

# Sequence tagging with probabilistic models
## Hidden Markov Models
### Problem
Given a sequence of tokens, infer the most probable sequence of labels for these tokens.

Ex. 
- part-of-speech (POS) tagging
  - Universal Dependencies project
- named entity recognition
- semantic slot filling

### Types of named entities
- persons, organizations, locations...
- dates and times, units, amounts...

### Approaches to sequence labeling
1. Rule-based models (EngCG tagger)
2. Separate label classifiers for each token (NB, LR)
3. Sequence models (HMM, MEMM, CRF)
4. NN

### PoS tagging with HMMs
Notation :
- x = x1...xt is a sequence of words (input)
- y = y1...yt is a sequence of tags (labels)

y = argmax P(y|x) = argmax P(x, y) (x is independent to y)

### Hidden Markov Model
![p(x,y)=p(x|y)p(y)\approx  \prod_{t=1}^T p(x_t|y_t)p(y_t|y_{t-1})](https://render.githubusercontent.com/render/math?math=p(x%2Cy)%3Dp(x%7Cy)p(y)%5Capprox%20%20%5Cprod_%7Bt%3D1%7D%5ET%20p(x_t%7Cy_t)p(y_t%7Cy_%7Bt-1%7D))

2 assumptions:
1. Markov assumption : 
![p(y)\approx  \prod_{t=1}^T p(y_t|y_{t-1})](https://render.githubusercontent.com/render/math?math=p(y)%5Capprox%20%20%5Cprod_%7Bt%3D1%7D%5ET%20p(y_t%7Cy_%7Bt-1%7D))

2. Output independence : 
![p(x|y)\approx  \prod_{t=1}^T p(x_t|y_t})](https://render.githubusercontent.com/render/math?math=p(x%7Cy)%5Capprox%20%20%5Cprod_%7Bt%3D1%7D%5ET%20p(x_t%7Cy_t%7D))

### Text generation in HMM
- choose the next PoS tag given the previous tag
- given the current tag, generates another word

### Formal definition of HMM
A hidden Markov Model is specified by
1. The set S = s1...sN of hidden states
2. The start state s0
3. The matrix A of transition probabilities : a_ij = p(s_j|s_i)
4. The set O of possible visible outcomes
5. The matrix B of output probabilities : b_ik = p(o_k|s_i)

a_ij = p(s_j|s_i) = c(s_i -> s_j)/c(s_i)

a_ik = p(o_k|s_i) = c(s_i -> o_k)/c(s_i)

### Baum-Welch algorithm (a sketch)
E-step : posterior probabilities for hidden variables : 
![p(y_{t-1}=s_i,y_t=s_j)](https://render.githubusercontent.com/render/math?math=p(y_%7Bt-1%7D%3Ds_i%2Cy_t%3Ds_j))

Can be done with dynamic programming (forward-backward algorithm)

M-step : maximum likelihood updates for the parameters
![a_ij=p(s_j|s_i)= \frac{\sum_{t=1}^T p(y_{t-1}=s_i,y_t=s_j)}{\sum_{t=1}^T p(y_t=s_i)} ](https://render.githubusercontent.com/render/math?math=a_ij%3Dp(s_j%7Cs_i)%3D%20%5Cfrac%7B%5Csum_%7Bt%3D1%7D%5ET%20p(y_%7Bt-1%7D%3Ds_i%2Cy_t%3Ds_j)%7D%7B%5Csum_%7Bt%3D1%7D%5ET%20p(y_t%3Ds_i)%7D%20)

## Viterbi algorithm: what are the most probable tags?
### Motivation
The same output sentence can be generated by different sequences of hidden states.
### Decoding in HMM
Decoding problem : what is the most probable sequence of hidden states => use dynamic programming
### Viterbi decoding
Find the most possible path. At each state, compare all possible ways and only keep the most possible one.

Complexity : change O(n^m) to O(m * n^2)

## MEMMs, CRFs and other sequential models for Named Entity Recognition
### HMM
![p(y,x)=\prod_{t=1}^T p(y_t|y_{t-1})p(x_t|y_t)](https://render.githubusercontent.com/render/math?math=p(y%2Cx)%3D%5Cprod_%7Bt%3D1%7D%5ET%20p(y_t%7Cy_%7Bt-1%7D)p(x_t%7Cy_t))
### MEMM (Maximum Entropy Markov Model)
![p(y|x)=\prod_{t=1}^T p(y_t|y_{t-1},x_t)](https://render.githubusercontent.com/render/math?math=p(y%7Cx)%3D%5Cprod_%7Bt%3D1%7D%5ET%20p(y_t%7Cy_%7Bt-1%7D%2Cx_t))

Discriminative model

### CRF (Conditional Random Field)


# Deep Learning for the same tasks
## Neural Language Models
### Curse of dimensionality
### How to generalize better
- Learn distributed representations for words
- Express probabilities of sequences and learn parameters
### Probabilistic Neural Language Model
- Softmax over components of y
- Feed-forward NN with tons of parameters
- Distributed representation of context words

